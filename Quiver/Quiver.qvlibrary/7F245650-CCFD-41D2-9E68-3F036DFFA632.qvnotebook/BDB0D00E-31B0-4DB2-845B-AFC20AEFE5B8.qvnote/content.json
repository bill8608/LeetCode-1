{
  "title": "Group Technical Question",
  "cells": [
    {
      "type": "markdown",
      "data": "## ArrayList\n\n- advantage\n  + calls to get and set take constant time\n- disadvantage\n  + insertion and removal might be expensive unless changes are made at the end of arraylist\n  + inefficient for search\n  + notation of capacity(size of underlying array)\n- operation\n  + get, set: O(1)\n  + insertion(add to front) and removal: O(n)\n\n\n## LinkedList\n- advantage\n  + insertion and removal is cheap(constant time), provided the position of the changes is known\n- disadvantage\n  + not easily indexable\n  + calls to get and set might be expensive\n  + inefficient for search\n- operation\n  + get, set: O(n)\n  + insertion, removal: O(1)\n\n## Stack\n- operation\n  + push: O(1)\n  + pop: O(1)\n- implementation\n  + list: push is to add node to the front of list and pop is to remove from front of list\n  + array: topOfStack is initialized to -1, when push, arr[topOfStack++]=element; when pop, return arr[--topOfStack]; use topOfStack==-1 to check if emtpy\n- applications\n  + balance symbols\n  + postfix expression\n  + infix to postfix conversion\n  + method calls\n\n## Queue\n- idea\n  + insertion is done at one end\n  + deletion is doen at the other end\n- operation\n  + enqueue: O(1)\n  + dequeue: O(1)\n  \n# Hashing\n\n- overview\n  + perform insertion, deletion, searching in average O(1) time\n  + print in sorted order in O(n) not supported\n  + load factor: ratio of number of elements in the hash table to the table size\n\n## hash function\n  Hashtable \n  Advantage: retreival and insertion in O(1) time \n  Disadvantage: Space complexity can increase if size of input is not known. \n  Use: when lot of retreival of values or keys are required. \n  \n  + determinism: given an input, always generate the same value\n  + uniformity: map inputs as evenly as possible over its output range\n  + defined range: desirable that outputs of hash function have fixed size\n  + continuity: a hash function that is used to search for similar data must be as continuous as possible, two inputs differing a little should be mapped to nearly equal hash values\n  + non-invertible: impossible to reconstruct input from hash value\n\n## collisions\n  + given inputs map to the same hash value\n  + separate chaining\n    - idea: keep a list elements that hash to the same value\n    - operations\n      + search: first determine which list to traverse and then search appropriate list\n      + insertion: first check appropriate list to see whether element is already in place(if duplicates allowed, keep an extra field); if element is new, insert into the front of list because recently inserted elements are more likely to be used in the near future\n    - disadvantage\n      + using linkedlist\n      + slow down time to allocate new cells\n      + require implementation of another data structure\n  + probing\n    - hash function: h_i(x) = (hash(x) + f(i)) % TABLE_SIZE\n    - types\n      + linear probing\n        - f(i) = i\n        - primary clustering: as long as table is large enough, a free cell can always be found but might take a long time; when table is relatively empty, blocks of occupied cells start forming\n        - insertions and unsuccessful search require the same number of probes\n        - on average, successful search takes less time than unsuccessful search\n      + quadratic probing\n        - f(i) = i^2\n        - eliminate the primary clustering\n        - no guarantee finding free cell when table gets more than half full or even before if table size is not prime because at most half of the table can be used as alternative locations to resolve collisions\n        - table size must be prime, otherwise insertion might fail\n        - secondary hashing\n          + solution by double hashing: f(i)=i*hash2(x)\n\n## rehashing\n  + when table gets too full, operations start taking too long\n  + solution: build another table with twice size, compute new hash value for new table\n  + running time: O(n)\n  + options\n    - rehash as soon as table is half full\n    - rehash only when an insertion fails\n    - rehash when table reaches a certain load factor\n\n## hash tables with worst O(1) time\n\n\n\n# Priority Queue(Heap)\n\n- structure property\n  + a heap is a binary tree that is completely filled(exception at the bottom level), the average height of complete binary tree is O(logN)\n  + for any element in array position i\n    - left child: 2i\n    - right child: 2i+1\n    - parent: floor(i/2)\n  + heap consists of Comparable objects\n- heap order property\n  + smallest(largest) element at the root\n  + min-heap: for every node X, key in the parent of X is <= key in X\n  + order property ensures findMin() in O(1) time\n- operations\n  + insert()\n    - create a hole in next available position, if order property not violated, done; otherwise, heapify up\n    - time O(logN) if inserted element is new minimum(maximum)\n  + deleteMin()\n    - find minimum is easy\n    - removing minimum will create a hole in the root, we must move last element X in the heap to correct position\n    - put X in correct spot along a path from the root containing minimum children(heapify down)\n    - time: worst O(logN), average O(logN)\n  + decreaseKey()\n    - lowers the value of item at position p by positive amount\n    - if violate order property, heapify up\n    - application: make something with higher priority\n  + increaseKey()\n    - increase the value of item at position p by positive amount\n    - if violate order property, heapify down\n    - application: scheduler drops the priority of a process that is consuming excessive CPU time\n  + delete()\n    - remove node at position p\n    - first perform decreaseKey(p,infinity) then perform deleteMin()\n    - application: process terminated by user\n  + buildHeap()\n    - done with N successive inserts\n    - insert takes O(1) average, O(logN) worst\n    - build takes O(N) average, O(NlogN) worst\n- applications\n  + selection problem\n    - approach one(k smallest elements)\n      + read N elements into array and buildHeap\n      + perform k deleteMin()\n      + time: buildHeap O(N), deleteMin O(logN), total is O(N+klogN)\n    - approach two(k largest elements)\n      + idea: at any time, maintain a set of k largest elements\n      + read first k elements\n      + when a new element is read, compare it with kth largest element Sk(Sk is the smallest element in set S)\n        - if new element is larger, it replaces Sk\n        - otherwise, do nothing\n      + finally, return the smallest element in S\n      + time: build with first k elements O(k), time to process remaining (N-k)O(logk), total is O(k+(N-k)logk) = O(Nlogk)\n  + event simulation\n"
    }
  ]
}